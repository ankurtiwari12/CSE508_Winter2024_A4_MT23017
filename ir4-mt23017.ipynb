{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18},{"sourceId":8174586,"sourceType":"datasetVersion","datasetId":4838594},{"sourceId":8194820,"sourceType":"datasetVersion","datasetId":4853774}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# df=pd.read_csv(\"C:\\\\Users\\\\ankur\\\\Reviews.csv\\\\Reviews.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_main= df[['Summary', 'Text']]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:19:23.169862Z","iopub.status.idle":"2024-04-22T21:19:23.170185Z","shell.execute_reply.started":"2024-04-22T21:19:23.170023Z","shell.execute_reply":"2024-04-22T21:19:23.170037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Download NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\ndef preprocess(text):\n    if isinstance(text,str):\n        text = BeautifulSoup(text, 'html.parser').get_text()\n        tokens = word_tokenize(text)\n        tokens = [token.lower() for token in tokens]\n        stop_words = set(stopwords.words('english'))\n        tokens = [token for token in tokens if token not in stop_words]\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n        tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens if token.strip() != '']\n        processed_text = ' '.join(tokens)\n        return processed_text\n    else:\n        return \"\"\ndf_main.loc[:,['Text', 'Summary']] = df_main[['Text', 'Summary']].applymap(preprocess)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_main = df_main.dropna().reset_index(drop=True)\n# df_main = df_main.drop_duplicates().reset_index(drop=True)\n# df_main.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_main = df_main[df_main['Summary']!='']\n# df_main = df_main[df_main['Text']!='']\n# df_main.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_main = df_main[['Summary', 'Text']].astype(str)\n# df_main","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Open the pickle file in binary mode for reading\nwith open('/kaggle/input/summary1/df_main.pkl', 'rb') as f:\n    # Load the data from the file\n    df_main = pickle.load(f)\n\n# Now, 'data' contains the Python object that was saved in the pickle file\ndf_main","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:27.336808Z","iopub.execute_input":"2024-04-22T21:58:27.337178Z","iopub.status.idle":"2024-04-22T21:58:29.156621Z","shell.execute_reply.started":"2024-04-22T21:58:27.337149Z","shell.execute_reply":"2024-04-22T21:58:29.155468Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                   Summary  \\\n0                    good quality dog food   \n1                        not as advertised   \n2                      delight says it all   \n3                           cough medicine   \n4                              great taffy   \n...                                    ...   \n394463                 will not do without   \n394464                        disappointed   \n394465            perfect for our maltipoo   \n394466  favorite training and reward treat   \n394467                         great honey   \n\n                                                     Text  \n0       i have bought several of the vitality canned d...  \n1       product arrived labeled as jumbo salted peanut...  \n2       this is a confection that has been around a fe...  \n3       if you are looking for the secret ingredient i...  \n4       great taffy at a great price there was a wide ...  \n...                                                   ...  \n394463  great for sesame chickenthis is a good if not ...  \n394464  im disappointed with the flavor the chocolate ...  \n394465  these stars are small so you can give 1015 of ...  \n394466  these are the best treats for training and rew...  \n394467  i am very satisfied product is as advertised i...  \n\n[394468 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>good quality dog food</td>\n      <td>i have bought several of the vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>not as advertised</td>\n      <td>product arrived labeled as jumbo salted peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>delight says it all</td>\n      <td>this is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cough medicine</td>\n      <td>if you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>great taffy</td>\n      <td>great taffy at a great price there was a wide ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>394463</th>\n      <td>will not do without</td>\n      <td>great for sesame chickenthis is a good if not ...</td>\n    </tr>\n    <tr>\n      <th>394464</th>\n      <td>disappointed</td>\n      <td>im disappointed with the flavor the chocolate ...</td>\n    </tr>\n    <tr>\n      <th>394465</th>\n      <td>perfect for our maltipoo</td>\n      <td>these stars are small so you can give 1015 of ...</td>\n    </tr>\n    <tr>\n      <th>394466</th>\n      <td>favorite training and reward treat</td>\n      <td>these are the best treats for training and rew...</td>\n    </tr>\n    <tr>\n      <th>394467</th>\n      <td>great honey</td>\n      <td>i am very satisfied product is as advertised i...</td>\n    </tr>\n  </tbody>\n</table>\n<p>394468 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_subset = df_main[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:31.194214Z","iopub.execute_input":"2024-04-22T21:58:31.194849Z","iopub.status.idle":"2024-04-22T21:58:31.201141Z","shell.execute_reply.started":"2024-04-22T21:58:31.194806Z","shell.execute_reply":"2024-04-22T21:58:31.200121Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"data = df_subset.reset_index(drop=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:31.606551Z","iopub.execute_input":"2024-04-22T21:58:31.606925Z","iopub.status.idle":"2024-04-22T21:58:31.622348Z","shell.execute_reply.started":"2024-04-22T21:58:31.606895Z","shell.execute_reply":"2024-04-22T21:58:31.621420Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                    Summary                                               Text\n0     good quality dog food  i have bought several of the vitality canned d...\n1         not as advertised  product arrived labeled as jumbo salted peanut...\n2       delight says it all  this is a confection that has been around a fe...\n3            cough medicine  if you are looking for the secret ingredient i...\n4               great taffy  great taffy at a great price there was a wide ...\n..                      ...                                                ...\n995                 not hot  not hot at all like the other low star reviewe...\n996    not hot not habanero  i have to admit i was a sucker for the large q...\n997              best babka  i never in my life tasted such a good babka it...\n998  my dog loves these but  i am so convinced these are human animal crack...\n999          she loves them  i have a whole box of peanut butter dog cookie...\n\n[1000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>good quality dog food</td>\n      <td>i have bought several of the vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>not as advertised</td>\n      <td>product arrived labeled as jumbo salted peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>delight says it all</td>\n      <td>this is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cough medicine</td>\n      <td>if you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>great taffy</td>\n      <td>great taffy at a great price there was a wide ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>not hot</td>\n      <td>not hot at all like the other low star reviewe...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>not hot not habanero</td>\n      <td>i have to admit i was a sucker for the large q...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>best babka</td>\n      <td>i never in my life tasted such a good babka it...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>my dog loves these but</td>\n      <td>i am so convinced these are human animal crack...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>she loves them</td>\n      <td>i have a whole box of peanut butter dog cookie...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pip install rouge\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:32.364832Z","iopub.execute_input":"2024-04-22T21:58:32.365182Z","iopub.status.idle":"2024-04-22T21:58:44.657532Z","shell.execute_reply.started":"2024-04-22T21:58:32.365155Z","shell.execute_reply":"2024-04-22T21:58:44.656214Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\nfrom sklearn.model_selection import train_test_split\nfrom rouge import Rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:44.660378Z","iopub.execute_input":"2024-04-22T21:58:44.660994Z","iopub.status.idle":"2024-04-22T21:58:44.674706Z","shell.execute_reply.started":"2024-04-22T21:58:44.660948Z","shell.execute_reply":"2024-04-22T21:58:44.671222Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Tokenize data\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.add_special_tokens({'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'})\ntokenizer.pad_token = '<PAD>'  # Set the padding token\n\n# Custom dataset\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = self.data.loc[idx, 'Text']\n        summary = f\"<BOS> {self.data.loc[idx, 'Summary']} <EOS>\"\n\n        review_encoding = self.tokenizer(review, truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n        summary_encoding = self.tokenizer(summary, truncation=True, max_length=128, padding='max_length', return_tensors='pt')\n\n        return {\n            'input_ids': review_encoding['input_ids'].squeeze(),\n            'attention_mask': review_encoding['attention_mask'].squeeze(),\n            'labels': summary_encoding['input_ids'].squeeze()\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:44.676157Z","iopub.execute_input":"2024-04-22T21:58:44.676595Z","iopub.status.idle":"2024-04-22T21:58:45.281250Z","shell.execute_reply.started":"2024-04-22T21:58:44.676560Z","shell.execute_reply":"2024-04-22T21:58:45.280223Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Split data\ndef split_data(data, test_size=0.25, random_state=42):\n    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n    return train_data, test_data\n\ntrain_data, test_data = split_data(data)\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\n# Create datasets and dataloaders\ntrain_dataset = ReviewDataset(train_data, tokenizer)\ntest_dataset = ReviewDataset(test_data, tokenizer)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=data_collator, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:45.283408Z","iopub.execute_input":"2024-04-22T21:58:45.283721Z","iopub.status.idle":"2024-04-22T21:58:45.296552Z","shell.execute_reply.started":"2024-04-22T21:58:45.283695Z","shell.execute_reply":"2024-04-22T21:58:45.295471Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)  # Resize the embedding layer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    evaluation_strategy='steps',\n    eval_steps=200,\n    logging_steps=10,\n    save_strategy='steps',\n    save_steps=200,\n    learning_rate=5e-5,\n)\n\n# Instantiate the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:58:45.297880Z","iopub.execute_input":"2024-04-22T21:58:45.298226Z","iopub.status.idle":"2024-04-22T22:04:11.020816Z","shell.execute_reply.started":"2024-04-22T21:58:45.298194Z","shell.execute_reply":"2024-04-22T22:04:11.019473Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [470/470 05:22, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>3.962100</td>\n      <td>4.065368</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.722800</td>\n      <td>4.016747</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=470, training_loss=4.686876613535779, metrics={'train_runtime': 322.8462, 'train_samples_per_second': 11.615, 'train_steps_per_second': 1.456, 'total_flos': 979845120000000.0, 'train_loss': 4.686876613535779, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"kaggle/working/save_model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:56:14.076677Z","iopub.status.idle":"2024-04-22T21:56:14.077183Z","shell.execute_reply.started":"2024-04-22T21:56:14.076927Z","shell.execute_reply":"2024-04-22T21:56:14.076949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data\ntest_data.to_csv(\"kaggle/working/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:56:14.078523Z","iopub.status.idle":"2024-04-22T21:56:14.078874Z","shell.execute_reply.started":"2024-04-22T21:56:14.078695Z","shell.execute_reply":"2024-04-22T21:56:14.078709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in test_loader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:04:11.022169Z","iopub.execute_input":"2024-04-22T22:04:11.022912Z","iopub.status.idle":"2024-04-22T22:04:11.226323Z","shell.execute_reply.started":"2024-04-22T22:04:11.022875Z","shell.execute_reply":"2024-04-22T22:04:11.225042Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[   72,   717,  5071,  ..., 50259, 50259, 50259],\n        [   72,  2904,  8155,  ..., 50259, 50259, 50259],\n        [ 4053,  2861,   262,  ..., 50259, 50259, 50259],\n        ...,\n        [   72, 18548,  1975,  ..., 50259, 50259, 50259],\n        [   69,   415,  3477,  ..., 50259, 50259, 50259],\n        [ 2655,  6819,   783,  ..., 50259, 50259, 50259]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[   72,   717,  5071,  ...,  -100,  -100,  -100],\n        [   72,  2904,  8155,  ...,  -100,  -100,  -100],\n        [ 4053,  2861,   262,  ...,  -100,  -100,  -100],\n        ...,\n        [   72, 18548,  1975,  ...,  -100,  -100,  -100],\n        [   69,   415,  3477,  ...,  -100,  -100,  -100],\n        [ 2655,  6819,   783,  ...,  -100,  -100,  -100]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\nmodel.to(device)\ngenerated_summaries = []\nreference_summaries = []\nfor index, row in tqdm(data.iterrows(), total=len(data), desc='processing data'):\n    if index == 538 :\n        continue  # Skip processing this index\n    if index== 859:\n        continue\n    review = row['Text']\n    summary = row['Summary']\n\n    # Skip empty summaries\n    if summary.strip() == '':\n        continue\n\n    # Tokenize review and summary\n    review_encoding = tokenizer(review, truncation=True, max_length=512, padding='max_length', return_tensors='pt')\n    summary_encoding = tokenizer(summary, truncation=True, max_length=128, padding='max_length', return_tensors='pt')\n\n    input_ids = review_encoding['input_ids'].to(device)\n    attention_mask = review_encoding['attention_mask'].to(device)\n\n    # Generate summary\n    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=128, num_return_sequences=1, early_stopping=True, pad_token_id=tokenizer.eos_token_id)\n    \n    if outputs is not None and len(outputs) > 0 and outputs[0] is not None:\n        # Decode generated summary and reference summary\n        tokens = []\n        for token_id in outputs[0]:\n            if token_id is not None:\n                decoded_token = tokenizer.decode(token_id.item())\n                if decoded_token == tokenizer.pad_token:\n                    break\n                tokens.append(decoded_token)\n\n        # Join tokens into a single string\n        generated_summary = \"\".join(tokens) if tokens else ''\n        reference_summary = summary\n\n        generated_summaries.append(generated_summary)\n        reference_summaries.append(reference_summary)\n    else:\n        # Handle case where outputs is None or empty\n        generated_summaries.append('')\n        reference_summaries.append('')\n\n# Compute ROUGE scores\nrouge = Rouge()\nscores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\nprint(scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:53:50.378080Z","iopub.execute_input":"2024-04-22T22:53:50.378833Z","iopub.status.idle":"2024-04-22T23:16:31.088543Z","shell.execute_reply.started":"2024-04-22T22:53:50.378799Z","shell.execute_reply":"2024-04-22T23:16:31.087573Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"processing data: 100%|██████████| 1000/1000 [22:40<00:00,  1.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"{'rouge-1': {'r': 0.49044381915123364, 'p': 0.048237567641080295, 'f': 0.08426655343227403}, 'rouge-2': {'r': 0.1596052345757639, 'p': 0.011530906407555615, 'f': 0.019990795503886345}, 'rouge-l': {'r': 0.4445283729852865, 'p': 0.04315792205103801, 'f': 0.0754225513985582}}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Accessing ROUGE-1 scores\nrouge_1_f1 = scores['rouge-1']['f']\nrouge_1_precision = scores['rouge-1']['p']\nrouge_1_recall = scores['rouge-1']['r']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:43.792164Z","iopub.execute_input":"2024-04-22T23:16:43.792527Z","iopub.status.idle":"2024-04-22T23:16:43.799581Z","shell.execute_reply.started":"2024-04-22T23:16:43.792500Z","shell.execute_reply":"2024-04-22T23:16:43.798471Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"rouge_1_f1","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:44.683420Z","iopub.execute_input":"2024-04-22T23:16:44.683788Z","iopub.status.idle":"2024-04-22T23:16:44.692001Z","shell.execute_reply.started":"2024-04-22T23:16:44.683762Z","shell.execute_reply":"2024-04-22T23:16:44.691040Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0.08426655343227403"},"metadata":{}}]},{"cell_type":"code","source":"# Input text and summary\ninput_text = \"The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound,and feels comfortable to play. However, some users have reported issues with the tuning stability.\"\ninput_summary = \"Good for beginners but has tuning stability issues.\"\n\n# Preprocess the input\ninput_encoding = tokenizer(input_text, truncation=True, max_length=512, padding='max_length', return_tensors='pt')\nsummary_encoding = tokenizer(input_summary, truncation=True, max_length=128, padding='max_length', return_tensors='pt')\n\ninput_ids = input_encoding['input_ids'].to(device)\nattention_mask = input_encoding['attention_mask'].to(device)\n\n# Generate summary\noutputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=128, num_return_sequences=1, early_stopping=True, pad_token_id=tokenizer.eos_token_id)\n\n# Decode generated summary\nif outputs[0] is not None:\n    tokens = []\n    for token_id in outputs[0]:\n        if token_id is not None:\n            decoded_token = tokenizer.decode(token_id.item())\n            if decoded_token == tokenizer.pad_token:\n                break\n            tokens.append(decoded_token)\n    generated_summary = \"\".join(tokens)\nelse:\n    generated_summary = \"\"\n\n# Compute ROUGE scores\nrouge_scores = rouge.get_scores(generated_summary, input_summary)\nprint(\"ROUGE Scores:\", rouge_scores)\n# # Decode generated summary\n# generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# # Compute ROUGE scores\n# rouge_scores = rouge.get_scores(generated_summary, input_summary)\n# print(\"ROUGE Scores:\", rouge_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:24:20.834657Z","iopub.execute_input":"2024-04-22T23:24:20.835591Z","iopub.status.idle":"2024-04-22T23:24:22.184345Z","shell.execute_reply.started":"2024-04-22T23:24:20.835555Z","shell.execute_reply":"2024-04-22T23:24:22.183279Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"ROUGE Scores: [{'rouge-1': {'r': 0.75, 'p': 0.18181818181818182, 'f': 0.29268292368828075}, 'rouge-2': {'r': 0.2857142857142857, 'p': 0.058823529411764705, 'f': 0.09756097277810835}, 'rouge-l': {'r': 0.625, 'p': 0.15151515151515152, 'f': 0.24390243588340277}}]\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_summary","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:24:37.333566Z","iopub.execute_input":"2024-04-22T23:24:37.334248Z","iopub.status.idle":"2024-04-22T23:24:37.341530Z","shell.execute_reply.started":"2024-04-22T23:24:37.334217Z","shell.execute_reply":"2024-04-22T23:24:37.340612Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"'The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound,and feels comfortable to play. However, some users have reported issues with the tuning stability.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}